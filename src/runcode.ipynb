{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000/100000 completed\n",
      "Episode 20000/100000 completed\n",
      "Episode 30000/100000 completed\n",
      "Episode 40000/100000 completed\n",
      "Episode 50000/100000 completed\n",
      "Episode 60000/100000 completed\n",
      "Episode 70000/100000 completed\n",
      "Episode 80000/100000 completed\n",
      "Episode 90000/100000 completed\n",
      "Episode 100000/100000 completed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from enums import CellState as cs\n",
    "from enums import GameState as gs\n",
    "\n",
    "class TicTacToe:\n",
    "\tdef __init__(self, length = 3):\n",
    "\t\tself.length = length\n",
    "\t\tself.grid = np.full((self.length,self.length), cs.EMPTY.value)\n",
    "\t\tself.moves = [(i, j) for i in range(self.length) for j in range(self.length)]\n",
    "\t\tself.game_state = gs.PLAYING.value\n",
    "\t\t\n",
    "\tdef reset(self):\n",
    "\t\tself.grid = np.full((self.length,self.length), cs.EMPTY.value)\n",
    "\t\tself.moves = [(i, j) for i in range(self.length) for j in range(self.length)]\n",
    "\t\tself.game_state = gs.PLAYING.value\n",
    "\t\treturn self.grid\n",
    "\n",
    "\t# def check_winner(self):\n",
    "\t# \twinner_found = False\n",
    "\n",
    "\t# \t# Check for rows and columns\n",
    "\t# \tfor i in range(self.length):\n",
    "\t# \t\tif np.all(self.grid[i, :] == cs.O.value) or np.all(self.grid[:, i] == cs.O.value):\n",
    "\t# \t\t\tself.game_state = cs.O.value\n",
    "\t# \t\t\twinner_found = True\n",
    "\t# \t\t\tbreak\n",
    "\t\t\t\n",
    "\t# \t\tif np.all(self.grid[i, :] == cs.X.value) or np.all(self.grid[:, i] == cs.X.value):\n",
    "\t# \t\t\tself.game_state = cs.X.value\n",
    "\t# \t\t\twinner_found = True\n",
    "\t# \t\t\tbreak\n",
    "\t\t\t\n",
    "\t# \t# Check for diagonals\n",
    "\t# \tif not winner_found:\n",
    "\t# \t\tfor i in range(self.length):\n",
    "\t# \t\t\tif np.all(self.grid[i, i] == cs.O.value) or \\\n",
    "\t# \t\t\t\tnp.all(self.grid[i, self.length - i - 1] == cs.O.value):\n",
    "\t# \t\t\t\tself.game_state = cs.O.value\n",
    "\t# \t\t\t\tbreak\n",
    "\n",
    "\t# \t\t\tif np.all(self.grid[i, i] == cs.X.value) or \\\n",
    "\t# \t\t\t\tnp.all(self.grid[i, self.length - i - 1] == cs.X.value):\n",
    "\t# \t\t\t\tself.game_state = cs.X.value\n",
    "\t# \t\t\t\tbreak\n",
    "\n",
    "\t# \treturn self.game_state\n",
    "\n",
    "\tdef check_winner(self, grid):\n",
    "\t\tfor row in grid:\n",
    "\t\t\tif row[0] == row[1] == row[2] and row[0] != 'EMPTY':\n",
    "\t\t\t\treturn row[0]  # Return 'x' or 'o'\n",
    "\n",
    "    # Check columns\n",
    "\t\tfor col in range(3):\n",
    "\t\t\tif grid[0][col] == grid[1][col] == grid[2][col] and grid[0][col] != 'EMPTY':\n",
    "\t\t\t\treturn grid[0][col]  # Return 'x' or 'o'\n",
    "\n",
    "    # Check diagonals\n",
    "\t\tif grid[0][0] == grid[1][1] == grid[2][2] and grid[0][0] != 'EMPTY':\n",
    "\t\t\treturn grid[0][0]  # Return 'x' or 'o'\n",
    "\t\t\n",
    "\t\tif grid[0][2] == grid[1][1] == grid[2][0] and grid[0][2] != 'EMPTY':\n",
    "\t\t\treturn grid[0][2]  # Return 'x' or 'o'\n",
    "\t\t\n",
    "\t\treturn gs.PLAYING.value  # No winner\n",
    "\t\n",
    "\n",
    "\tdef step(self, player, move): \n",
    "\t\twinner_reward = 10 #Kind of arbitrary and dependent on what we want to do.\n",
    "\t\ttie_winner_reward = 3 #Should be low, but not too low, since with optimal play, there is a tie.\n",
    "\t\tcenter_reward = 2\n",
    "\t\tcorner_reward = 1\n",
    "\t\tremaining_reward = -0.5\n",
    "\n",
    "\t\tself.grid[move] = player #Set grid based on move.\n",
    "\n",
    "\t\tself.game_state = self.check_winner(self.grid) #Made chatGPT make this shitty ass function above.\n",
    "\n",
    "\t\t#Winning statement. Return grid and winner upon a winner being discovered, or upon a tie. Our \"state\" is both the grid and the winner.\n",
    "\t\tif self.game_state == gs.AGENT_WIN.value or self.game_state == gs.PLAYER_WIN.value:\n",
    "\t\t\treturn self.grid, self.game_state, winner_reward, True\n",
    "\t\telif not (cs.EMPTY.value in self.grid.flatten()): #If self.actions is EMPTY, this will be true.\n",
    "\t\t\treturn self.grid, gs.TIE.value, tie_winner_reward if player == 1 else -tie_winner_reward, True\n",
    "\t\telse: #Here should go the rewards for particular actions, like putting in a center square. Maybe we start with no rewards at first?\n",
    "\t\t\tif move == (1,1):\n",
    "\t\t\t\treturn self.grid, gs.PLAYING.value, center_reward, False\n",
    "\t\t\telif move == (0,0) or move == (2,0) or move == (0,2) or move == (2,2):\n",
    "\t\t\t\treturn self.grid, gs.PLAYING.value, corner_reward, False\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn self.grid, gs.PLAYING.value, remaining_reward, False\n",
    "\t\t\n",
    "#That should be the end of the environment itself.\n",
    "def array_to_tuple(array):\n",
    "    \"\"\"Convert an NxN array into a tuple of tuples.\"\"\"\n",
    "    return tuple(map(tuple, array))\n",
    "\n",
    "def tuple_to_array(tpl):\n",
    "    \"\"\"Convert a tuple of tuples back into an NxN array.\"\"\"\n",
    "    return np.array(tpl)\n",
    "\n",
    "class QLearningAgent:\n",
    "\tdef __init__(self, env, learning_rate = 0.1, discount_factor=0.9, exploration_rate=0.2):\n",
    "\t\tself.env = env #Environment HAHA\n",
    "\t\tself.q_table = {} #Make dynamic list.\n",
    "\t\tself.learning_rate = learning_rate  # Alpha: learning rate\n",
    "\t\tself.discount_factor = discount_factor  # Gamma: discount factor for future rewards\n",
    "\t\tself.exploration_rate = exploration_rate  # Epsilon: exploration rate for epsilon-greedy\n",
    "\t\n",
    "\tdef choose_move(self, state):\n",
    "\t\tif random.uniform(0, 1) < self.exploration_rate:\n",
    "\t\t\tavailable_moves_indexes = np.where(state.flatten() == cs.EMPTY.value)[0]\n",
    "\t\t\tnew_moves = [self.env.moves[i] for i in available_moves_indexes]\n",
    "\t\t\treturn random.choice(new_moves)\n",
    "\t\telse:\n",
    "\t\t\t# Choose the action with the highest Q-value\n",
    "\t\t\treturn self.best_move(state)\n",
    "\t\n",
    "\tdef valid_moves(self, state):\n",
    "\t\tavailable_moves_indexes = np.where(state.flatten() == cs.EMPTY.value)[0]\n",
    "\t\treturn [self.env.moves[i] for i in available_moves_indexes]\n",
    "\t\n",
    "\tdef best_move(self,state):\n",
    "\t\tnew_moves = self.valid_moves(state)\n",
    "\t\tq_values = np.array([self.get_q_value(state,move) for move in new_moves])\n",
    "\t\treturn new_moves[np.argmax(q_values)]\n",
    "\t\n",
    "\tdef get_q_value(self,state, move):\n",
    "\t\treturn self.q_table.get((array_to_tuple(state), move), 0)\n",
    "\n",
    "\tdef update_q_value(self, state, move, reward, next_state):\n",
    "\t\told_q_value = self.get_q_value(state,move)\n",
    "\t\tnew_moves = self.valid_moves(next_state)\n",
    "\t\tnext_best_q_value = max([self.get_q_value(next_state, newmove) for newmove in new_moves], default = 0)\n",
    "\t\tnew_q_value = old_q_value + self.learning_rate * (reward + self.discount_factor * next_best_q_value - old_q_value)\n",
    "\t\tself.q_table[(array_to_tuple(state), move)] = new_q_value\n",
    "\n",
    "\n",
    "#Training:\n",
    "def opponent_moves(grid, moves):\n",
    "\tavailable_moves_indexes = np.where(grid.flatten() == cs.EMPTY.value)[0]\n",
    "\tnew_moves = [moves[i] for i in available_moves_indexes]\n",
    "\treturn random.choice(new_moves)\n",
    "\n",
    "\n",
    "def train_q(episodes = 1000, print_interval = 1000):\n",
    "\tenv = TicTacToe()\n",
    "\tagent = QLearningAgent(env)\n",
    "\n",
    "\tfor episode in range(episodes):\n",
    "\t\tcurrent_grid = agent.env.reset() + 0\n",
    "\t\tdone = False\n",
    "\t\t\n",
    "\t\twhile not done:\n",
    "\t\t\tmove = agent.choose_move(current_grid)\n",
    "\t\t\tnext_grid, winner, reward, done = agent.env.step(cs.O.value, move)\n",
    "\t\t\tagent.update_q_value(current_grid, move, reward, next_grid)\n",
    "\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\tcurrent_grid = next_grid + 0\n",
    "\n",
    "\t\t\topponent_move = opponent_moves(current_grid, agent.env.moves)\n",
    "\t\t\tnext_grid, winner, reward, done = agent.env.step(cs.X.value, opponent_move)\n",
    "\n",
    "\t\t\tagent.update_q_value(current_grid, move, -reward, next_grid)\n",
    "\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tcurrent_grid = next_grid + 0\n",
    "\t\t\n",
    "\t\tif (episode + 1) % print_interval == 0:\n",
    "\t\t\tprint(f\"Episode {episode + 1}/{episodes} completed\")\n",
    "\n",
    "\treturn agent\n",
    "\n",
    "printN = 10\n",
    "N = printN*10000\n",
    "ag = train_q(episodes=N, print_interval=int(N/printN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.202335569231275"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag.q_table[(((1, 0, 1), (2, 0, 2), (0, 0, 0)), (0, 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_values = [(i,j) for i in range(3) for j in range(3)]\n",
    "index_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 2], [0, 1, 2], [0, 0, 0]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = [[1, 0, 2], [0, 1, 2], [0, 0, 0]]\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position is: [[1, 0, 2], [0, 1, 2], [0, 0, 0]], q-val for (0, 1) is: -0.4999992277308201\n",
      "Position is: [[1, 0, 2], [0, 1, 2], [0, 0, 0]], q-val for (1, 0) is: -0.4999977851536227\n",
      "Position is: [[1, 0, 2], [0, 1, 2], [0, 0, 0]], q-val for (2, 0) is: 0.9999998631085208\n",
      "Position is: [[1, 0, 2], [0, 1, 2], [0, 0, 0]], q-val for (2, 1) is: -0.49999662422439073\n",
      "Position is: [[1, 0, 2], [0, 1, 2], [0, 0, 0]], q-val for (2, 2) is: 9.999999999999993\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(index_values)):\n",
    "  try:\n",
    "  \tprint(f\"Position is: {position}, q-val for {index_values[i]} is: {ag.q_table[(array_to_tuple(position), index_values[i])]}\")\n",
    "  except:\n",
    "    pass\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original board:\n",
      "O |   | X\n",
      "-----\n",
      "  | O | X\n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "\n",
      "Board after move 1:\n",
      "O | O | X\n",
      "-----\n",
      "  | O | X\n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "q_value is: -0.4999992277308201\n",
      "\n",
      "Board after move 2:\n",
      "O |   | X\n",
      "-----\n",
      "O | O | X\n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "q_value is: -0.4999977851536227\n",
      "\n",
      "Board after move 3:\n",
      "O |   | X\n",
      "-----\n",
      "  | O | X\n",
      "-----\n",
      "O |   |  \n",
      "-----\n",
      "q_value is: 0.9999998631085208\n",
      "\n",
      "Board after move 4:\n",
      "O |   | X\n",
      "-----\n",
      "  | O | X\n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "q_value is: -0.49999662422439073\n",
      "\n",
      "Board after move 5:\n",
      "O |   | X\n",
      "-----\n",
      "  | O | X\n",
      "-----\n",
      "  |   | O\n",
      "-----\n",
      "q_value is: 9.999999999999993\n",
      "\n",
      "Best move is move 5!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_board(board):\n",
    "    \"\"\"Prints the Tic-Tac-Toe board in a readable format.\"\"\"\n",
    "    symbols = {0: \" \", 1: \"O\", 2: \"X\"}\n",
    "    for row in board:\n",
    "        print(\" | \".join(symbols[cell] for cell in row))\n",
    "        print(\"-\" * 5)\n",
    "\n",
    "def apply_moves(model, board, player):\n",
    "    \"\"\"Applies all available moves (empty spots, marked as 0) to the Tic-Tac-Toe board.\"\"\"\n",
    "    available_boards = []\n",
    "    q_vals = []\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if board[i][j] == 0:  # Find an available spot\n",
    "                new_board = board.copy()  # Copy the board\n",
    "                new_board[i][j] = player  # Place an 'X' (represented by 2)\n",
    "                available_boards.append(new_board)\n",
    "                q_vals.append(model.q_table[(array_to_tuple(position), (i,j))])\n",
    "    \n",
    "    return available_boards, q_vals\n",
    "\n",
    "# Example input board\n",
    "input_board = np.array([[1, 0, 2], [0, 1, 2], [0, 0, 0]])\n",
    "\n",
    "# Print the board before applying moves\n",
    "print(\"Original board:\")\n",
    "print_board(input_board)\n",
    "\n",
    "# Apply all available moves\n",
    "available_boards, q_vals = apply_moves(ag, input_board, 1)\n",
    "\n",
    "# Print the boards after applying the moves\n",
    "for idx, board in enumerate(available_boards):\n",
    "    print(f\"\\nBoard after move {idx + 1}:\")\n",
    "    print_board(board)\n",
    "    print(f\"q_value is: {q_vals[idx]}\")\n",
    "\n",
    "print()\n",
    "print(f\"Best move is move {np.argmax(q_vals)+1}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
